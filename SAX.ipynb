{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efstathios Chatziloizos - SAX paper experiment replication\n",
    "[A Symbolic Representation of Time Series, with Implications for\n",
    "Streaming Algorithms](https://www.cs.ucr.edu/~eamonn/SAX.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load [Control Chart dataset](https://archive.ics.uci.edu/dataset/139/synthetic+control+chart+time+series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_synthetic_control(filename):\n",
    "    \"\"\"\n",
    "    Loads the Synthetic Control Chart dataset from a text file.\n",
    "    Each of the 600 lines has 60 space-separated floats.\n",
    "    Classes (in order):\n",
    "        0: normal        (lines 0..99)\n",
    "        1: cyclic        (lines 100..199)\n",
    "        2: increasing    (lines 200..299)\n",
    "        3: decreasing    (lines 300..399)\n",
    "        4: upward shift  (lines 400..499)\n",
    "        5: downward shift(lines 500..599)\n",
    "    Returns:\n",
    "        X: list of lists (600 x 60) of floats\n",
    "        y: list of int (600)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        values = list(map(float, line.strip().split()))\n",
    "        X.append(values)\n",
    "        label = idx // 100\n",
    "        y.append(label)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = load_synthetic_control(\"datasets/synthetic_control.data\")\n",
    "\n",
    "samples_per_class = 2\n",
    "\n",
    "# We'll store color or style for each class\n",
    "colors = [\"red\", \"green\", \"blue\", \"orange\", \"purple\", \"brown\"]\n",
    "class_names = [\n",
    "    \"Normal (0)\",\n",
    "    \"Cyclic (1)\",\n",
    "    \"Increasing (2)\",\n",
    "    \"Decreasing (3)\",\n",
    "    \"Upward shift (4)\",\n",
    "    \"Downward shift (5)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for c in range(6):\n",
    "    # Find indices of items that belong to class c\n",
    "    indices_c = [i for i, label in enumerate(y) if label == c]\n",
    "    # Take just a few\n",
    "    for idx in indices_c[:samples_per_class]:\n",
    "        plt.plot(X[idx], label=f\"Class {c}\", color=colors[c], alpha=0.7)\n",
    "\n",
    "plt.title(\"Synthetic Control Chart - Sample Time Series\")\n",
    "plt.xlabel(\"Time index (0..59)\")\n",
    "plt.ylabel(\"Value\")\n",
    "\n",
    "handles = []\n",
    "for c in range(6):\n",
    "    line = plt.Line2D([], [], color=colors[c], label=class_names[c])\n",
    "    handles.append(line)\n",
    "plt.legend(handles=handles)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAX implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_breakpoints(max_a):\n",
    "    \"\"\"\n",
    "    Compute breakpoints for Gaussian distribution to ensure equiprobable regions for SAX discretization.\n",
    "    Returns a dictionary where keys are alphabet sizes and values are lists of breakpoints.\n",
    "    \"\"\"\n",
    "    breakpoints = {}\n",
    "    for a in range(3, max_a + 1):\n",
    "        breakpoints[a] = [norm.ppf(i / a) for i in range(1, a)]\n",
    "    return breakpoints\n",
    "\n",
    "# Precompute breakpoints for Gaussian distribution, ensuring equiprobable regions for SAX discretization.\n",
    "BREAKPOINTS = compute_breakpoints(32) # allow up to a 32-letter alphabet\n",
    "\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    \"\"\"Compute Euclidean distance between two lists a and b.\"\"\"\n",
    "    return math.dist(a, b)\n",
    "\n",
    "def z_normalize(ts):\n",
    "    \"\"\"Z-normalize a time series ts (list of floats).\"\"\"\n",
    "    n = len(ts)\n",
    "    mean_val = sum(ts) / n\n",
    "    sq_sum = sum((val - mean_val)**2 for val in ts)\n",
    "    std_val = math.sqrt(sq_sum / n)\n",
    "    if std_val < 1e-12:\n",
    "        return [0.0] * n\n",
    "    return [(val - mean_val) / std_val for val in ts]\n",
    "\n",
    "def paa(ts, w):\n",
    "    \"\"\"\n",
    "    Piecewise Aggregate Approximation of ts into w segments.\n",
    "    Assumes len(ts) is divisible by w for simplicity.\n",
    "    \"\"\"\n",
    "    n = len(ts)\n",
    "    step = n // w\n",
    "    return [sum(ts[i*step:(i+1)*step]) / step for i in range(w)]\n",
    "\n",
    "def ts_to_sax(ts, w, a):\n",
    "    \"\"\"\n",
    "    Convert a time series ts to SAX:\n",
    "      1) Z-normalize\n",
    "      2) PAA of length w\n",
    "      3) Map each segment to a symbol in [0..a-1]\n",
    "    \"\"\"\n",
    "    znorm = z_normalize(ts)\n",
    "    paa_vals = paa(znorm, w)\n",
    "    bpts = BREAKPOINTS[a]\n",
    "    symbols = []\n",
    "    for val in paa_vals:\n",
    "        symbol = 0\n",
    "        while symbol < a - 1 and val > bpts[symbol]:\n",
    "            symbol += 1\n",
    "        symbols.append(symbol)\n",
    "    return symbols\n",
    "\n",
    "def symbolic_dist(sym_i, sym_j, breakpoints):\n",
    "    \"\"\"\n",
    "    Distance between two symbols sym_i, sym_j in SAX.\n",
    "    If gap == 0 or 1 => 0, else difference of breakpoints.\n",
    "    \"\"\"\n",
    "    if sym_i == sym_j:\n",
    "        return 0.0\n",
    "    gap = abs(sym_i - sym_j)\n",
    "    if gap == 1:\n",
    "        return 0.0\n",
    "    left = min(sym_i, sym_j)\n",
    "    right = max(sym_i, sym_j)\n",
    "    return breakpoints[right - 1] - breakpoints[left]\n",
    "\n",
    "def sax_mindist(sax_word1, sax_word2, a, original_length, w):\n",
    "    \"\"\"\n",
    "    Lower-bound distance between two SAX words, as per the SAX paper.\n",
    "    MINDIST = sqrt( (original_length / w) * sum(symbolic_dist^2 ) ).\n",
    "    \"\"\"\n",
    "    bpts = BREAKPOINTS[a]\n",
    "    sum_sq = 0.0\n",
    "    for i in range(w):\n",
    "        d = symbolic_dist(sax_word1[i], sax_word2[i], bpts)\n",
    "        sum_sq += d * d\n",
    "    return math.sqrt((original_length / w) * sum_sq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering (Section 4.1 of the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "def compute_distance_matrix(data, dist_func):\n",
    "    \"\"\"\n",
    "    Given a list of items `data` (each item is a time series or SAX word),\n",
    "    and a distance function dist_func(i, j) => float,\n",
    "    build an NxN distance matrix in the form required by `scipy`.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    # Build a 1D array of size n*(n-1)/2 in row-major (condensed) form\n",
    "    # First fill up a standard NxN matrix, then convert with squareform.\n",
    "    dist_mat = [[0.0]*n for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            d = dist_func(data[i], data[j])\n",
    "            dist_mat[i][j] = d\n",
    "            dist_mat[j][i] = d\n",
    "    # Convert NxN -> condensed\n",
    "    # squareform wants a 1D list of upper-triangular values\n",
    "    condensed = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            condensed.append(dist_mat[i][j])\n",
    "    return condensed\n",
    "\n",
    "def hierarchical_clustering_euclidean(X_sub):\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering (complete linkage) using\n",
    "    Euclidean distance on a subset X_sub of time series.\n",
    "    Returns the linkage matrix Z from scipy.\n",
    "    \"\"\"\n",
    "    # Build condensed distance array\n",
    "    dist_condensed = compute_distance_matrix(X_sub, euclidean_distance)\n",
    "    Z = sch.linkage(dist_condensed, method='complete')\n",
    "    return Z\n",
    "\n",
    "def hierarchical_clustering_sax(X_sub, w, a):\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering (complete linkage) using\n",
    "    SAX MINDIST on a subset X_sub of time series.\n",
    "    Returns the linkage matrix Z from scipy.\n",
    "    \"\"\"\n",
    "    # Convert time series to SAX\n",
    "    original_length = len(X_sub[0])  # 60 for control chart\n",
    "    sax_data = [ts_to_sax(ts, w, a) for ts in X_sub]\n",
    "\n",
    "    # Store indices\n",
    "    indices = list(range(len(sax_data)))\n",
    "\n",
    "    def sax_index_dist(i_idx, j_idx):\n",
    "        return sax_mindist(sax_data[i_idx], sax_data[j_idx], a, original_length, w)\n",
    "\n",
    "    dist_condensed = compute_distance_matrix(indices, sax_index_dist)\n",
    "    Z = sch.linkage(dist_condensed, method='complete')\n",
    "    return Z\n",
    "\n",
    "# Load the data\n",
    "X, y = load_synthetic_control(\"datasets/synthetic_control.data\")\n",
    "\n",
    "# Pick 9 series (3 from each of these classes: normal=0, decreasing=3, upward shift=4) (as per the paper)\n",
    "desired_classes = [0, 3, 4]  # normal, decreasing, upward shift\n",
    "\n",
    "X_sub, y_sub = [], []\n",
    "for c in desired_classes:\n",
    "    # get indices for class c\n",
    "    indices_c = [i for i, label in enumerate(y) if label == c]\n",
    "    # take first 3\n",
    "    for idx in indices_c[:3]:\n",
    "        X_sub.append(X[idx])\n",
    "        y_sub.append(y[idx])\n",
    "\n",
    "# Hierarchical clustering with Euclidean\n",
    "Z_euc = hierarchical_clustering_euclidean(X_sub)\n",
    "\n",
    "# Hierarchical clustering with SAX\n",
    "# The paper doesn't specify the exact w/a used for that dendrogram,\n",
    "w = 15\n",
    "a = 13\n",
    "Z_sax = hierarchical_clustering_sax(X_sub, w, a)\n",
    "\n",
    "# Plot side-by-side dendrograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: Euclidean\n",
    "sch.dendrogram(Z_euc, ax=axes[0])\n",
    "axes[0].set_title(\"Hierarchical Clustering (Euclidean)\\n(3 classes x 3 samples)\")\n",
    "\n",
    "# Right: SAX\n",
    "sch.dendrogram(Z_sax, ax=axes[1])\n",
    "axes[1].set_title(f\"Hierarchical Clustering (SAX: w={w}, a={a})\\n(3 classes x 3 samples)\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_ylabel(\"Distance\")\n",
    "    ax.set_xlabel(\"Sample Index\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with SAX\n",
    "### 1 Nearest Neighbor Classification (Section 4.2.1 of the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_1nn_sax(X, y, w, a):\n",
    "    \"\"\"\n",
    "    1-NN classification using the SAX MINDIST (Leave-One-Out).\n",
    "    Returns accuracy in [0..1].\n",
    "    \"\"\"\n",
    "    original_length = len(X[0])\n",
    "    sax_data = [ts_to_sax(ts, w, a) for ts in X]\n",
    "    correct = 0\n",
    "    total = len(X)\n",
    "    for i in range(total):\n",
    "        best_dist = float('inf')\n",
    "        best_label = None\n",
    "        for j in range(total):\n",
    "            if i == j:\n",
    "                continue\n",
    "            dist = sax_mindist(sax_data[i], sax_data[j], a, original_length, w)\n",
    "            if dist < best_dist:\n",
    "                best_dist = dist\n",
    "                best_label = y[j]\n",
    "        if best_label == y[i]:\n",
    "            correct += 1\n",
    "    return correct / total  # fraction of correct classifications\n",
    "\n",
    "def do_1nn_euclidean(X, y):\n",
    "    \"\"\"\n",
    "    Performs 1-NN classification using Euclidean distance (Leave-One-Out).\n",
    "    Returns the accuracy as a fraction in [0..1].\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = len(X)\n",
    "    for i in range(total):\n",
    "        best_dist = float('inf')\n",
    "        best_label = None\n",
    "        for j in range(total):\n",
    "            if i == j:\n",
    "                continue\n",
    "            dist = euclidean_distance(X[i], X[j])\n",
    "            if dist < best_dist:\n",
    "                best_dist = dist\n",
    "                best_label = y[j]\n",
    "        if best_label == y[i]:\n",
    "            correct += 1\n",
    "    return correct / total  # e.g. 0.95 means 95% accuracy\n",
    "\n",
    "# Load the Control Chart dataset\n",
    "X, y = load_synthetic_control(\"datasets/synthetic_control.data\")\n",
    "\n",
    "# Euclidean error rate (constant for all a)\n",
    "frac_acc_euc = do_1nn_euclidean(X, y)\n",
    "error_euc = 1 - frac_acc_euc\n",
    "print(f\"Euclidean Error Rate: {error_euc:.3f}\")\n",
    "\n",
    "# For SAX, we fix w=15 (4:1 dimension reduction from 60) as mentioned in the paper.\n",
    "w = 15\n",
    "\n",
    "# Run SAX for best found parameters of a = 13 and w = 15.\n",
    "a = 13\n",
    "frac_acc_sax = do_1nn_sax(X, y, w, a)\n",
    "error_sax = 1 - frac_acc_sax\n",
    "\n",
    "print(f\"SAX Error Rate (w={w}, a={a}): {error_sax:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreate Figure 13 of the paper for the Control Chart dataset\n",
    "\n",
    "The experiment is extended for alphabet sizes larger than 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_euc = do_1nn_euclidean(X, y)\n",
    "error_euclidean = 1 - acc_euc\n",
    "\n",
    "w = 15\n",
    "sax_error_rates = []\n",
    "alphabet_sizes = range(5, 14)\n",
    "\n",
    "for a in alphabet_sizes:\n",
    "    acc_sax = do_1nn_sax(X, y, w, a)\n",
    "    error_sax = 1 - acc_sax\n",
    "    sax_error_rates.append(error_sax)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "# Euclidean: just one horizontal line\n",
    "plt.plot(alphabet_sizes, [error_euclidean]*len(alphabet_sizes),\n",
    "            label=\"Euclidean\", marker='o', linestyle='--')\n",
    "\n",
    "# SAX line\n",
    "plt.plot(alphabet_sizes, sax_error_rates, label=\"SAX\", marker='s')\n",
    "plt.axvline(x=10, color='red', linestyle=':', linewidth=1)\n",
    "\n",
    "plt.title(\"Control Chart - 1-NN Error Rate vs. Alphabet Size\")\n",
    "plt.xlabel(\"Alphabet Size (a)\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.ylim(0, 0.6)  # error rates from 0 to 0.6 as in the paper\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query by Content (Indexing) with SAX (Section 4.3 of the paper)\n",
    "\n",
    "### Indexing on the Control Chart dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sax_query(query_ts, X, y, w, a, k=10):\n",
    "    \"\"\"\n",
    "    Find the nearest neighbor for `query_ts` using SAX-based candidate pruning.\n",
    "    \"\"\"\n",
    "    # Convert entire dataset to SAX\n",
    "    sax_data = [ts_to_sax(ts, w, a) for ts in X]\n",
    "    query_sax = ts_to_sax(query_ts, w, a)\n",
    "    original_length = len(query_ts)\n",
    "    \n",
    "    # Compute MINDIST between query and all SAX words\n",
    "    mindist = []\n",
    "    for sax_word in sax_data:\n",
    "        dist = sax_mindist(query_sax, sax_word, a, original_length, w)\n",
    "        mindist.append(dist)\n",
    "    \n",
    "    # Get indices of top `k` candidates based on MINDIST\n",
    "    top_k_indices = sorted(range(len(mindist)), key=lambda i: mindist[i])[:k]\n",
    "    \n",
    "    # Refine with actual Euclidean distance on original data\n",
    "    query_znorm = z_normalize(query_ts)\n",
    "    min_euc = float('inf')\n",
    "    best_idx = -1\n",
    "    for idx in top_k_indices:\n",
    "        candidate_znorm = z_normalize(X[idx])\n",
    "        dist = euclidean_distance(query_znorm, candidate_znorm)\n",
    "        if dist < min_euc:\n",
    "            min_euc = dist\n",
    "            best_idx = idx\n",
    "    \n",
    "    return best_idx, min_euc\n",
    "\n",
    "# Load the Synthetic Control dataset\n",
    "X, y = load_synthetic_control(\"datasets/synthetic_control.data\")\n",
    "\n",
    "# Pick a sample time series (e.g., first \"Normal\" class)\n",
    "sample_ts = X[0]\n",
    "\n",
    "# SAX parameters\n",
    "w = 12\n",
    "a = 7\n",
    "\n",
    "# Z-normalize the sample\n",
    "znorm = z_normalize(sample_ts)\n",
    "\n",
    "# Compute PAA\n",
    "paa_vals = paa(znorm, w)\n",
    "\n",
    "# Convert PAA to SAX symbols\n",
    "sax_symbols = ts_to_sax(sample_ts, w, a)\n",
    "\n",
    "# Map symbols to letters (0->a, 1->b, etc.)\n",
    "sax_letters = [chr(ord('a') + s) for s in sax_symbols]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(znorm, label=\"Z-normalized\", alpha=0.7)\n",
    "\n",
    "# Plot PAA segments\n",
    "step = len(znorm) // w\n",
    "for i in range(w):\n",
    "    start = i * step\n",
    "    end = (i+1) * step\n",
    "    plt.hlines(paa_vals[i], start, end-1, colors='red', linestyles='dashed', linewidth=2)\n",
    "\n",
    "# Annotate SAX symbols\n",
    "for i in range(w):\n",
    "    x_pos = (i * step) + (step // 2)\n",
    "    plt.text(x_pos, paa_vals[i] + 0.1, sax_letters[i], ha='center', color='green', fontsize=12)\n",
    "\n",
    "# Plot breakpoints\n",
    "breakpoints = BREAKPOINTS[a]\n",
    "for bp in breakpoints:\n",
    "    plt.axhline(bp, color='green', linestyle='dotted', alpha=0.5)\n",
    "\n",
    "plt.title(f\"SAX Representation (w={w}, a={a})\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Query all time series and keep statistics of how many it got right\n",
    "correct_sax = 0\n",
    "\n",
    "for i in range(len(X)):\n",
    "    query = X[i]\n",
    "    best_idx, sax_dist = sax_query(query, X, y, w=w, a=a, k=10)\n",
    "    \n",
    "    if y[best_idx] == y[i]:\n",
    "        correct_sax += 1\n",
    "\n",
    "print(f\"SAX-based NN accuracy: {correct_sax / len(X) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower bounding Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Setup: Synthetic dataset + Query\n",
    "\n",
    "def generate_synthetic_dataset(n_series=200, length=60, seed=42):\n",
    "    \"\"\"\n",
    "    Generate a random dataset of n_series time series, each of given length.\n",
    "    We'll also assign a random 'class' label just for demonstration.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(n_series):\n",
    "        # Create a random walk or random time series\n",
    "        ts = np.cumsum(np.random.randn(length))\n",
    "        X.append(ts)\n",
    "        y.append(i % 3)  # example label\n",
    "    return X, y\n",
    "\n",
    "# 1-NN with Euclidean distance (full scan - always read from disk)\n",
    "def one_nn_euclidean_full_scan(query_ts, dataset):\n",
    "    \"\"\"\n",
    "    Naive 1-NN with Euclidean distance.\n",
    "    We simulate that each time we check a candidate, we do 1 disk read.\n",
    "    Returns (best_index, best_dist, disk_reads).\n",
    "    \"\"\"\n",
    "    best_idx = -1\n",
    "    best_dist = float('inf')\n",
    "    disk_reads = 0\n",
    "    query_znorm = z_normalize(query_ts)\n",
    "    \n",
    "    for i, ts in enumerate(dataset):\n",
    "        # Simulate reading from disk\n",
    "        disk_reads += 1\n",
    "        \n",
    "        candidate_znorm = z_normalize(ts)\n",
    "        dist = euclidean_distance(query_znorm, candidate_znorm)\n",
    "        if dist < best_dist:\n",
    "            best_dist = dist\n",
    "            best_idx = i\n",
    "    return best_idx, best_dist, disk_reads\n",
    "\n",
    "# SAX-based pruning: keep SAX in memory, only read from disk if MINDIST is < best_dist_so_far.\n",
    "def one_nn_sax_pruning(query_ts, dataset, sax_dataset, w, a):\n",
    "    \"\"\"\n",
    "    Single-level SAX approach.\n",
    "      1) Convert query_ts to SAX\n",
    "      2) For each candidate's SAX, compute MINDIST\n",
    "      3) If MINDIST < best_dist, read from disk => compute real Eucl dist\n",
    "    Returns (best_idx, best_dist, disk_reads, pruned_count).\n",
    "    pruned_count: how many were pruned by MINDIST alone (no disk read).\n",
    "    \"\"\"\n",
    "    query_sax = ts_to_sax(query_ts, w, a)\n",
    "    query_znorm = z_normalize(query_ts)\n",
    "    best_idx = -1\n",
    "    best_dist = float('inf')\n",
    "    disk_reads = 0\n",
    "    pruned_count = 0  # how many we can prune due to MINDIST >= best_dist\n",
    "    \n",
    "    for i, sax_word in enumerate(sax_dataset):\n",
    "        dist_lb = sax_mindist(query_sax, sax_word, a, orig_len, w)\n",
    "        if dist_lb < best_dist:\n",
    "            disk_reads += 1\n",
    "            candidate_znorm = z_normalize(dataset[i])\n",
    "            dist = euclidean_distance(query_znorm, candidate_znorm)\n",
    "            if dist < best_dist:\n",
    "                best_dist = dist\n",
    "                best_idx = i\n",
    "        else:\n",
    "            # pruned by MINDIST\n",
    "            pruned_count += 1\n",
    "    \n",
    "    return best_idx, best_dist, disk_reads, pruned_count\n",
    "\n",
    "\n",
    "# Demo: Compare 1-NN with Euclidean vs. SAX-based pruning\n",
    "def demo_comparison(num_queries=10, track_latency=None):\n",
    "    \"\"\"\n",
    "    Dor random queries from X, do:\n",
    "      - Full Euclidean 1-NN\n",
    "      - Single-level SAX 1-NN\n",
    "    Compute stats and print.\n",
    "    \"\"\"\n",
    "    # Randomly pick some queries\n",
    "    indices = random.sample(range(len(X)), num_queries)\n",
    "    \n",
    "    # We accumulate stats\n",
    "    total_disk_euclid = 0\n",
    "    total_disk_sax = 0\n",
    "\n",
    "    # Time\n",
    "    euclid_time = 0\n",
    "    sax_time = 0\n",
    "\n",
    "    # Latency: 12ms per disk read and 100 microsec for ssd\n",
    "    hdd_latency = 0.012  # 12ms in seconds\n",
    "    ssd_latency = 0.0001  # 100 microsec in seconds\n",
    "    latency_euclid = 0\n",
    "    latency_sax = 0\n",
    "    \n",
    "    correct_matches = 0  # optional: how often they pick the same best index\n",
    "    for q_idx in indices:\n",
    "        q_ts = X[q_idx]\n",
    "        \n",
    "        # Full Euclidean\n",
    "        start_time = time.time()\n",
    "        e_idx, e_dist, e_reads = one_nn_euclidean_full_scan(q_ts, X)\n",
    "        euclid_time += time.time() - start_time\n",
    "\n",
    "        # SAX-based\n",
    "        start_time = time.time()\n",
    "        s_idx, s_dist, s_reads, pruned_count = one_nn_sax_pruning(q_ts, X, sax_data, w, a)\n",
    "        sax_time += time.time() - start_time\n",
    "\n",
    "        # Add the would-be latency for disk reads\n",
    "        if track_latency == 'hdd':\n",
    "            latency_euclid += e_reads * hdd_latency\n",
    "            latency_sax += s_reads * hdd_latency\n",
    "        else:\n",
    "            latency_euclid+= e_reads * ssd_latency\n",
    "            latency_sax += s_reads * ssd_latency\n",
    "        \n",
    "        \n",
    "        total_disk_euclid += e_reads\n",
    "        total_disk_sax += s_reads\n",
    "        \n",
    "        if e_idx == s_idx:\n",
    "            correct_matches += 1\n",
    "\n",
    "    if track_latency:        \n",
    "        # Add latency to total time\n",
    "        euclid_time += latency_euclid\n",
    "        sax_time += latency_sax\n",
    "\n",
    "    \n",
    "    avg_euclid = total_disk_euclid / num_queries\n",
    "    avg_sax = total_disk_sax / num_queries\n",
    "    same_nn_rate = correct_matches / num_queries\n",
    "    if same_nn_rate != 1:\n",
    "        # Sanity check that there is not bug in the code (they should always pick the same best index)\n",
    "        print(f\"Warning: Not all queries found the same best index.\")\n",
    "    \n",
    "    print(f\"-- Results over {num_queries} queries (track_latency={track_latency}) --\")\n",
    "    if track_latency:\n",
    "        print(f\"  Full Euclidean: avg disk reads = {avg_euclid:.2f}\\t time = {euclid_time:.2f}s, of which {latency_euclid:.2f}s is {track_latency} access latency ({(latency_euclid/euclid_time)*100:.2f})%\")\n",
    "        print(f\"  SAX-based:      avg disk reads = {avg_sax:.2f}\\t time = {sax_time:.2f}s, of which {latency_sax:.2f}s is {track_latency} access latency ({(latency_sax/sax_time)*100:.2f})%\")\n",
    "    else:\n",
    "        print(f\"  Full Euclidean: avg disk reads = {avg_euclid:.2f}\\t time = {euclid_time:.2f}s\")\n",
    "        print(f\"  SAX-based:      avg disk reads = {avg_sax:.2f}\\t time = {sax_time:.2f}s\")\n",
    "    print(f\"  SAX pruned {pruned_count} candidates, or {(pruned_count / (avg_sax + pruned_count)) * 100:.2f}% of total candidates\")\n",
    "    print(f\"  Time reduction: {(1 - (sax_time / euclid_time)) * 100:.2f}% and latency reduction: {(1- (latency_sax / latency_euclid)) * 100:.2f}%\")\n",
    "\n",
    "    # Dimensinality reduction from real-valued time series to SAX words\n",
    "    quotient = len(X[0]) / w\n",
    "    formatted_quotient = f\"{int(quotient)}\" if quotient.is_integer() else f\"{quotient:.1f}\"\n",
    "    print(f\"Dimensionality reduction: {len(X[0])} -> {w}, so {formatted_quotient} to 1\")\n",
    "\n",
    "\n",
    "# Generate Data\n",
    "X, y = generate_synthetic_dataset(n_series=50000, length=600, seed=42)\n",
    "\n",
    "\n",
    "# Parameters for SAX\n",
    "w = 15\n",
    "a = 10\n",
    "orig_len = len(X[0])\n",
    "\n",
    "# Precompute the SAX words for the entire dataset\n",
    "sax_data = [ts_to_sax(ts, w, a) for ts in X]\n",
    "\n",
    "# Run a small demo, NOTE: to track latency, use track_latency='hdd' or 'ssd', default is None\n",
    "demo_comparison(num_queries=1, track_latency='ssd')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: Two‐Level SAX Approach for Indexing\n",
    "\n",
    "\n",
    "In many scenarios, using a single SAX representation may still lead to a relatively large number of disk reads (refinement steps). To reduce these further, we tried out a two‐level SAX approach whose idea is to use a very coarse SAX representation to prune many candidates early on, before refining the distance estimate with a more detailed SAX representation.\n",
    "\n",
    "1. **Coarse Representation**  \n",
    "   We choose a smaller $w_{coarse}$ (number of PAA segments) and a smaller $\\alpha_{coarse}$(alphabet size) to create a very compressed symbolic approximation.\n",
    "   - This coarse representation is very cheap to store in memory and to compare (MINDIST is computed quickly).\n",
    "   - It can prune many obviously distant candidates early on, without any disk reads.\n",
    "\n",
    "2. **Refined Representation**  \n",
    "  If a candidate time series survives the coarse check, we then compare it with a refined SAX representation, using larger $w_{fine}$ and $\\alpha_{fine}$.\n",
    "   - This yields a tighter MINDIST bound, pruning borderline candidates that the coarse representation could not confidently eliminate.\n",
    "   - Still, no disk reads have happened yet for those pruned at this stage.\n",
    "\n",
    "1. **Final Euclidean Check**  \n",
    "   Only if the refined MINDIST is still below the best distance so far do we fetch the candidate’s raw time series from disk and compute the actual Euclidean distance. For a good choice of **Refined Representation** only a small fraction of candidates will make it to this final step, saving significant I/O and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_nn_sax_twolevel(query_ts, dataset, sax_data_coarse, w_coarse, a_coarse, sax_data_fine, w_fine, a_fine, orig_len):\n",
    "    \"\"\"\n",
    "    Two-level SAX approach:\n",
    "      1) Coarse MINDIST => prune quickly with smaller (w_coarse, a_coarse)\n",
    "      2) If passes, refine with MINDIST_fine => (w_fine, a_fine)\n",
    "      3) If still passes, read from disk => full Eucl dist\n",
    "    Returns (best_idx, best_dist, disk_reads, coarse_pruned_count, fine_pruned_count).\n",
    "    \"\"\"\n",
    "    query_sax_coarse = ts_to_sax(query_ts, w_coarse, a_coarse)\n",
    "    query_sax_fine   = ts_to_sax(query_ts, w_fine,   a_fine)\n",
    "    query_znorm      = z_normalize(query_ts)\n",
    "    \n",
    "    best_idx = -1\n",
    "    best_dist = float('inf')\n",
    "    disk_reads = 0\n",
    "    \n",
    "    coarse_pruned_count = 0\n",
    "    fine_pruned_count   = 0\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        # Coarse MINDIST\n",
    "        dist_lb_coarse = sax_mindist(query_sax_coarse, sax_data_coarse[i], a_coarse, orig_len, w_coarse)\n",
    "        if dist_lb_coarse >= best_dist:\n",
    "            # Prune: no disk read\n",
    "            coarse_pruned_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Fine MINDIST\n",
    "        dist_lb_fine = sax_mindist(query_sax_fine, sax_data_fine[i], a_fine, orig_len, w_fine)\n",
    "        if dist_lb_fine >= best_dist:\n",
    "            # Prune: no disk read\n",
    "            fine_pruned_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Still promising => do real Euclidean => 1 disk read\n",
    "        disk_reads += 1\n",
    "        candidate_znorm = z_normalize(dataset[i])\n",
    "        dist = euclidean_distance(query_znorm, candidate_znorm)\n",
    "        if dist < best_dist:\n",
    "            best_dist = dist\n",
    "            best_idx = i\n",
    "    \n",
    "    return best_idx, best_dist, disk_reads, coarse_pruned_count, fine_pruned_count\n",
    "\n",
    "# Demo comparison of 1-NN with Euclidean vs. Single-level vs. Two-level SAX\n",
    "def demo_comparison(num_queries=5, track_latency=None):\n",
    "    \"\"\"\n",
    "    For random queries from X, do:\n",
    "      - Full Euclidean 1-NN\n",
    "      - Single-level SAX 1-NN\n",
    "      - Two-level SAX 1-NN\n",
    "    Compute stats and print.\n",
    "    \"\"\"\n",
    "    indices = random.sample(range(len(X)), num_queries)\n",
    "    \n",
    "    # Stats\n",
    "    total_disk_euclid = 0\n",
    "    total_disk_sax_single = 0\n",
    "    total_disk_sax_two = 0\n",
    "\n",
    "    # For time\n",
    "    euclid_time = 0\n",
    "    sax_single_time = 0\n",
    "    sax_two_time = 0\n",
    "\n",
    "    # For latency\n",
    "    hdd_latency = 0.012  # 12ms in seconds\n",
    "    ssd_latency = 0.0001 # 100 microseconds in seconds\n",
    "    lat_euclid = 0\n",
    "    lat_sax_single = 0\n",
    "    lat_sax_two = 0\n",
    "\n",
    "    # For correctness checks\n",
    "    correct_single = 0\n",
    "    correct_two = 0\n",
    "\n",
    "    # For pruning stats\n",
    "    # Single-level: we track how many we skip from MINDIST\n",
    "    pruned_single_total = 0\n",
    "    \n",
    "    # Two-level: we track how many pruned at coarse vs fine\n",
    "    coarse_pruned_total = 0\n",
    "    fine_pruned_total   = 0\n",
    "\n",
    "    # Each query does a full pass => len(X) comparisons\n",
    "    # so total comparisons for single-level is num_queries * len(X).\n",
    "    # same for two-level.\n",
    "    n_candidates = len(X)\n",
    "\n",
    "    for q_idx in indices:\n",
    "        query_ts = X[q_idx]\n",
    "\n",
    "        # Full Euclidean  \n",
    "        t0 = time.time()\n",
    "        e_idx, e_dist, e_reads = one_nn_euclidean_full_scan(query_ts, X)\n",
    "        t1 = time.time()\n",
    "        euclid_time += (t1 - t0)\n",
    "\n",
    "        if track_latency == 'hdd':\n",
    "            lat_euclid += e_reads * hdd_latency\n",
    "        elif track_latency == 'ssd':\n",
    "            lat_euclid += e_reads * ssd_latency\n",
    "        \n",
    "        total_disk_euclid += e_reads\n",
    "        \n",
    "        # Single-Level SAX \n",
    "        t0 = time.time()\n",
    "        s_idx, s_dist, s_reads, s_pruned = one_nn_sax_pruning(query_ts, X, sax_data_fine, w_fine, a_fine)\n",
    "        t1 = time.time()\n",
    "        sax_single_time += (t1 - t0)\n",
    "\n",
    "        if track_latency == 'hdd':\n",
    "            lat_sax_single += s_reads * hdd_latency\n",
    "        elif track_latency == 'ssd':\n",
    "            lat_sax_single += s_reads * ssd_latency\n",
    "        \n",
    "        total_disk_sax_single += s_reads\n",
    "        pruned_single_total += s_pruned\n",
    "\n",
    "        if s_idx == e_idx:\n",
    "            correct_single += 1\n",
    "\n",
    "        # Two-Level SAX \n",
    "        t0 = time.time()\n",
    "        two_idx, two_dist, two_reads, c_pruned, f_pruned = one_nn_sax_twolevel(query_ts, X, sax_data_coarse, w_coarse, a_coarse, sax_data_fine, w_fine, a_fine, orig_len)\n",
    "        t1 = time.time()\n",
    "        sax_two_time += (t1 - t0)\n",
    "\n",
    "        if track_latency == 'hdd':\n",
    "            lat_sax_two += two_reads * hdd_latency\n",
    "        elif track_latency == 'ssd':\n",
    "            lat_sax_two += two_reads * ssd_latency\n",
    "        \n",
    "        total_disk_sax_two += two_reads\n",
    "\n",
    "        coarse_pruned_total += c_pruned\n",
    "        fine_pruned_total += f_pruned\n",
    "\n",
    "        if two_idx == e_idx:\n",
    "            correct_two += 1\n",
    "\n",
    "    # Add simulated latency to CPU times\n",
    "    euclid_time += lat_euclid\n",
    "    sax_single_time += lat_sax_single\n",
    "    sax_two_time += lat_sax_two\n",
    "\n",
    "    # Summaries\n",
    "    avg_euclid_reads = total_disk_euclid / num_queries\n",
    "    avg_sax_single_reads = total_disk_sax_single / num_queries\n",
    "    avg_sax_two_reads = total_disk_sax_two / num_queries\n",
    "\n",
    "    # Compute total comparisons for single-level = num_queries * n_candidates\n",
    "    total_single_comparisons = num_queries * n_candidates\n",
    "    total_two_comparisons = num_queries * n_candidates\n",
    "\n",
    "    # single-level pruned ratio\n",
    "    single_prune_ratio = (pruned_single_total / total_single_comparisons) * 100\n",
    "\n",
    "    # two-level pruned ratio\n",
    "    #   - coarse pruned ratio\n",
    "    #   - fine pruned ratio (only among those that survived coarse)\n",
    "    # The total that reached Eucl is (two_reads).\n",
    "    # so coarse_pruned_total + fine_pruned_total + two_reads = total_two_comparisons\n",
    "    two_coarse_ratio = (coarse_pruned_total / total_two_comparisons) * 100\n",
    "    two_fine_ratio   = (fine_pruned_total   / total_two_comparisons) * 100\n",
    "    # The remainder are the ones that do final Eucl check:\n",
    "    two_eucl_count   = total_two_comparisons - (coarse_pruned_total + fine_pruned_total)\n",
    "    two_eucl_ratio   = (two_eucl_count / total_two_comparisons) * 100\n",
    "\n",
    "    # Compare times\n",
    "    time_reduction_single = (1 - (sax_single_time / euclid_time)) * 100\n",
    "    time_reduction_two    = (1 - (sax_two_time    / euclid_time)) * 100\n",
    "\n",
    "    print(f\"-- Results over {num_queries} queries (track_latency={track_latency}) --\")\n",
    "    print(f\"Dataset size: {len(X)} time series, each length={orig_len}\")\n",
    "    print(f\"Full Euclid: avg disk reads = {avg_euclid_reads:.2f}, total time = {euclid_time:.3f}s\")\n",
    "\n",
    "    print(f\"  Single-SAX: avg disk reads = {avg_sax_single_reads:.2f}, total time = {sax_single_time:.3f}s\")\n",
    "    print(f\"    => Pruned {pruned_single_total} out of {total_single_comparisons} comparisons ({single_prune_ratio:.1f}%)\\n\")\n",
    "    print(f\"  TwoLvl-SAX: avg disk reads = {avg_sax_two_reads:.2f}, total time = {sax_two_time:.3f}s\")\n",
    "    print(f\"    => Coarse pruned {coarse_pruned_total}  ({two_coarse_ratio:.1f}%)\")\n",
    "    print(f\"    => Fine pruned   {fine_pruned_total}  ({two_fine_ratio:.1f}%)\")\n",
    "    print(f\"    => Final Eucl check  {two_eucl_count}  ({two_eucl_ratio:.1f}%)\")\n",
    "\n",
    "    print(f\"Time Reduction vs Euclid: single={time_reduction_single:.1f}%, twolevel={time_reduction_two:.1f}%\")\n",
    "    # Print the dimensionality reduction\n",
    "    quotient_coarse = orig_len / w_coarse\n",
    "    quotient_fine = orig_len / w_fine\n",
    "    formatted_coarse = f\"{int(quotient_coarse)}\" if quotient_coarse.is_integer() else f\"{quotient_coarse:.1f}\"\n",
    "    formatted_fine = f\"{int(quotient_fine)}\" if quotient_fine.is_integer() else f\"{quotient_fine:.1f}\"\n",
    "    print(f\"Dimensionality reduction from raw to coarse SAX: {orig_len} -> {w_coarse}, so {formatted_coarse} to 1\")\n",
    "    print(f\"Dimensionality reduction from raw to fine SAX: {orig_len} -> {w_fine}, so {formatted_fine} to 1\")\n",
    "\n",
    "\n",
    "# Generate Data\n",
    "X, y = generate_synthetic_dataset(n_series=25000, length=600, seed=42)\n",
    "orig_len = len(X[0])  # e.g. 600\n",
    "\n",
    "# \"Fine\" SAX parameters (Big one)\n",
    "w_fine = 60\n",
    "a_fine = 10\n",
    "\n",
    "# \"Coarse\" SAX parameters (very small w/a => minimal memory)\n",
    "w_coarse = 8\n",
    "a_coarse = 4\n",
    "\n",
    "# Precompute the two sets of SAX words:\n",
    "sax_data_fine   = [ts_to_sax(ts, w_fine,   a_fine)   for ts in X]\n",
    "sax_data_coarse = [ts_to_sax(ts, w_coarse, a_coarse) for ts in X]\n",
    "\n",
    "# Run Demo\n",
    "demo_comparison(num_queries=1, track_latency='ssd')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
